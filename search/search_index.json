{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NLP Concepts","text":"Terminology <ul> <li>Corpus: A large collection of text documents or spoken language data used for training and testing NLP models</li> <li>Dimensionality: Number of dimensions in a word vector. A high-dimensional word vector would have many dimensions, allowing it to capture a wide range of characteristics and nuances of the word.</li> <li>Embedding: Vectorization in multi dimensional space. To give tokens meaning, the model must be trained on them. This allows the model to learn the meanings of words and how they relate to other words. To achieve this, the word vectors are \u201cembedded\u201d into an embedding space.</li> <li>Named Entity Recognition(NER): A named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. Also see spaCy NER.</li> <li>Pipelines: The steps used for processing a document and extract the constituent building blocks. Also see spaCy Pipelines</li> <li>Stemming / Lemmatization: Reducing words to their base or root form to handle variations of the same word, such as singular/plural forms or verb tenses</li> <li>Stopwords: Commonly used words (determinants, conjunctions, prepositions, pronouns, auxillary verbs etc.) in a language but do not carry much meaning or significance. </li> <li>Tokenization: The process of parsing a statement and representing them as the constituent words or punctuations.  For example, dividing text by whitespace is one common approach, but there are many others. The purpose of tokenization is to create a vocabulary from a corpus. Each token is assigned a unique id to represent it as a number. Also see spaCy Tokenization</li> <li>Transformers: Deep learning models that are trained on vectors to understand the meaning of words and how they relate to each other.</li> <li>Vector / Word Vector: Vector representation of a word in NLP. Each word vector can be thought of as a point in a multi-dimensional space, where each dimension represents a particular aspect or characteristic of the word. Combining all the dimensions in a vector allows the model to understand the word\u2019s meaning and how it relates to other words.</li> <li>Vectorization: Machine learning models typically require the data/features to be numeric. The process of transforming non-numeric data such as tokens into numeric features is called vectorization. During vectorization, the unique ids in the tokens are assigned to randomly initialized n-dimensional vectors.</li> <li>Vocabulary: Set of unique tokens found within the corpus</li> </ul>"},{"location":"#building-blocks","title":"Building Blocks","text":"<ul> <li>Tokenization: Breaking text into tokens (words, sentences, n-grams)</li> <li>Stop-word removal: a/an/the</li> <li>Stemming and lemmatization: root word and word relation detection</li> <li>TF-IDF (Term Frequency Inverse Document Frequency): word importance/uniqueness with regards to a given document</li> <li>Part-of-speech tagging: noun/verb/adjective</li> <li>Noun Phrase Extraction:</li> <li>NER (Named Entity Recognition): person/organization/location</li> <li>Spelling correction: \"Santa Calra Country\"</li> <li>Word sense disambiguation: \"Apple stock price\"</li> <li>Segmentation: \"San Francisco City subway\"</li> <li>Language detection: \"translate this page\"</li> <li>Sentiment Analysis:</li> </ul>"},{"location":"#tokenization","title":"Tokenization","text":"<ul> <li>Character tokenization - Treats each character as a separate token</li> <li>Word tokenization - Divides the text into individual words or even subwords</li> <li>Subword tokenization - Combines the benefits of character and word tokenization by breaking down rare words into smaller units while keeping frequent words as unique entities.<ul> <li>Allows the model to handle complex words and misspellings while keeping the length of the inputs manageable.</li> <li>Typically 1 token ~ 4 characters or ~ 0.75 words</li> </ul> </li> <li>Sentence tokenization - Divides the text into individual sentences</li> </ul>"},{"location":"#vectorization","title":"Vectorization","text":""},{"location":"#bag-of-words","title":"Bag Of Words","text":"<p>This is the simplest vectorization technique and involves three operations:</p> <ul> <li>Tokenization: The input is represented as a list of its constituent words, disregarding grammar and word order.<ul> <li>It is called <code>bag of words</code> because the document's structure is lost \u2014 as if the words are all jumbled up in a bag.</li> </ul> </li> <li>Vocabulary creation: Of all the obtained tokenized words, only unique words are selected to create the vocabulary.<ul> <li>CountVectorizer creates a dictionary called <code>vocabulary_</code> that converts each word to its index in the sparse matrix.</li> </ul> </li> <li>Vector creation: A sparse matrix is created out of the frequency of vocabulary words. In this sparse matrix, each row is a sentence vector whose length (the columns of the matrix) is equal to the size of the vocabulary.<ul> <li>The words are the features(columns) and the feature vectorization just involves marking the feature as a 0 or 1 depending on whether it appears in the document or not</li> <li>Another approach is to count the number of times each word (feature) appears in the document.</li> <li>The vectorization will be mostly zeros, since only a few unique words are in each document.<ul> <li>For that reason, instead of storing all the zeros we only store non-zero values inside the 'sparse matrix' data structure.</li> </ul> </li> </ul> </li> </ul>"},{"location":"#common-vectorization-options","title":"Common Vectorization Options","text":""},{"location":"#n-grams","title":"N-Grams","text":"<p>Allows us to specify whether a group of words should be considered as a single feature</p> <ul> <li>We can specify a lower and upper boundary of the range of n-values for different n-grams to be extracted</li> <li>Adds all word combinations as specified by the lower and upper boundaries<ul> <li>For example, lower=1, upper=2 will include al single and double word combinations (multiple word combinations are only for consecutive words)</li> </ul> </li> <li>When we produce n-grams from a document with  \ud835\udc4a  words, we add a maximum of \\(\ud835\udc5b\u2212\ud835\udc4a+1\\) features. </li> <li>May lead to <code>feature explosion</code></li> </ul>"},{"location":"#topic-modeling","title":"Topic Modeling","text":"<p>A frequently used approach to discover hidden semantic patterns portrayed by a text corpus and automatically identify topics that exist inside it.</p> <ul> <li>A type of statistical modeling that leverages <code>unsupervised machine learning</code> to analyze and identify clusters or groups of similar words within a body of text.</li> <li>Topics are the latent descriptions of a corpus of text<ul> <li>Documents related to a specific topic are more likely to produce certain words more frequently</li> </ul> </li> <li>Two popular topic modeling techniques:  Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)</li> </ul>"},{"location":"#latent-semantic-analysis-lsa","title":"Latent Semantic Analysis (LSA)","text":"<p>A natural language processing technique used to analyze relationships between documents and the terms they contain. </p> <ul> <li>Assumes words with similar meanings will appear in similar documents. </li> <li>Constructs a matrix containing the word counts per document, where each row represents a unique word, and columns represent each document</li> <li>Uses Singular Value Decomposition (SVD) to reduce the number of rows while preserving the similarity structure among columns</li> <li>Cosine similarity is used to determine the similarity between documents<ul> <li>A value close to 1 means the documents are very similar based on the words in them</li> <li>A value close to 0 means they're quite different</li> </ul> </li> </ul>"},{"location":"#latent-dirichlet-allocation-lda","title":"Latent Dirichlet Allocation (LDA)","text":"<p>A natural language technique used to identify topics a document belongs to based on the words contained within it.</p> <ul> <li>It is a Bayesian network i.e., it\u2019s a generative statistical model that assumes documents are made up of words that aid in determining the topics</li> <li>Documents are mapped to a list of topics by assigning each word in the document to different topics</li> <li>This model ignores the order of words occurring in a document and treats them as a bag of words</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>Terminology</p> <ul> <li>GPT: Generative Pre-Trained Transformers</li> <li>GAN: Generative Adversarial Networks</li> <li>VAE: Variational Autoencoders (Used in anomaly detection)</li> <li>RLHF: Reinforcement Learning Human Feedback</li> <li>RAG: Retrieval Augmented Generation</li> <li>Multimodal AI: Allows inputs from muliple modalities (i.e. different types of data) such as images, video, audio and text</li> </ul>"},{"location":"concepts/#custom-gpts","title":"Custom GPTs","text":"<ul> <li>Creating customized GPTs by pre-setting instructions<ul> <li>Can be reused without having to repeat the instructions</li> <li>Can be invoked using mentions</li> </ul> </li> </ul>"},{"location":"concepts/#llms","title":"LLMs","text":""},{"location":"concepts/#api-parameters","title":"API Parameters","text":"<ul> <li>Max Tokens: Determines the maximum number of tokens that can be generated. This parameter helps control the verbosity of the response. The value typically ranges between 0 and 2048, though it can vary depending on the model and context.</li> <li>Temperature: Controls the randomness of the output. It influences how creative (less predictive responses) or deterministic (more predictive responses) the responses are. The value ranges between 0 and 2. The default value is 0.8.</li> <li>Top P (Nucleus Sampling): Dictates the variety in responses by only considering the top \u2018P\u2019 percent of probable words. It is an alternative to sampling with temperature and controls the diversity of the generated text. The value ranges between 0 and 1. The default value is 0.95.</li> <li>Frequency Penalty: Reduces repetition by decreasing the likelihood of frequently used words. It penalizes new tokens based on their existing frequency in the text so far. The value ranges between -2.0 and 2.0. This setting is disabled by default (value 0)</li> <li>Presence Penalty: Promotes the introduction of new topics in the conversation. It penalizes new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. The value ranges between -2.0 and 2.0. Positive values encourage diverse ideas and minimize repetition. This setting is disabled by default (value 0)</li> </ul> Temperature and Top P sample values Use Case Temperature Top P Description Chatbot 0.5 0.5 Generates responses that balance coherence and diversity resulting in a more natural and engaging conversational tone. Code Generation 0.2 0.1 Generates code that adheres to established patterns and conventions. Code output is more focussed and syntactically correct. Code Comment Generation 0.3 0.2 Generates concise and relevant code comments that adhere to conventions. Creative Writing 0.7 0.8 Generates creative and diverse text less constrained by patterns. Data Analysis Scripting 0.2 0.1 Generates focussed, efficient and correct analysis scripts. Exploratory Code Writing 0.6 0.7 Generates creative code that considers and explores multiple solutions."},{"location":"concepts/#training","title":"Training","text":"<ul> <li>Three phases of training:<ul> <li>Pre Training <ul> <li>Model is trained on large volumes of text</li> <li>Results in the creation of a parameter file <ul> <li>For example the LLama2 model is trained on 10 TB of trxt and produces a compressed 140 GB parameter file. </li> </ul> </li> <li>Open source LLMs have a run file in addition to the parameter file.<ul> <li>The run file contains some code to run the parameter file.<ul> <li>Written in C or Python</li> </ul> </li> </ul> </li> </ul> </li> <li>Fine Tuning <ul> <li>LLMs can be fine tuned with hundreds of thousands (roughly 100,000 in general) of high-quality prompts and completions (questions and expected answers)</li> <li>Enhances the model's efficiency </li> <li>Allows models to respond without hallucinations</li> <li>Allows superior results with less input and potentially using smaller models</li> <li>ChatGPT uses the SGD optmizer (Stoichastic Gradient Descent) for fine tuning</li> </ul> </li> <li>Reinforcement Training <ul> <li>Let the model know how good the response is and the model learns based on the feedback.</li> </ul> </li> </ul> </li> </ul>"},{"location":"concepts/#types","title":"Types","text":"<ul> <li>Base <ul> <li>Provide some basic instructions or examples <ul> <li>Examples can be in terms of how to respond (A) when something is asked (Q)</li> </ul> </li> <li>Ask the desired question at the end and wait for the LLM to respond</li> <li>Can be useful for fine tuning to learn new skills</li> </ul> </li> <li>Chat / Instruct<ul> <li>Provide context in addition to the examples, otherwise similar to Base LLMs</li> <li>Can be used for interactive use cases<ul> <li>Content generation</li> </ul> </li> </ul> </li> <li>Reasoning / Thinking<ul> <li>Include instruction to make the LLM think through and explain the steps</li> <li>Adding the word <code>wait</code> in the instructions can make the LLM analyze its thought process<ul> <li>Can result in better outputs</li> </ul> </li> <li>More useful for problem solving</li> <li>Uses Inference Time Scaling<sup>1</sup> ( as opposed to Training Time Scaling)</li> </ul> </li> </ul>"},{"location":"concepts/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<ul> <li>Components for RAG Implementation<ul> <li>Document Ingestion<ul> <li>Ingestion</li> <li>Transformation and Storage <ul> <li>Required to handle the <code>context size</code> limitations of the LLMs<ul> <li>Once the LLM reaches the limit, it is not going to remember the previous content that was discussed with it</li> </ul> </li> <li>Transformation involves Splitting the input into Chunks</li> <li>Embedding <ul> <li>Input is converted to tokens which are then conversion to Vectors</li> <li>Retains some reference to the original content the embeddings were created from</li> <li>Aids in Similarity Search</li> </ul> </li> <li>Storage in Vector Database (FAISS, ChromaDB, AstraDB etc.)<ul> <li>Vector embeddings stored as clusters in 3d space</li> </ul> </li> </ul> </li> </ul> </li> <li>User Query and Response<ul> <li>User query<ul> <li>Create embdedings from query using the same embedding model</li> </ul> </li> <li>Pre defined prompt</li> <li>Retrieval Chain<ul> <li>Interface for querying Vector Store DB</li> <li>Perform Similarity Search</li> </ul> </li> <li>Prompt + User Query + Enhanced context from Retrieval Chain sent to LLM</li> <li>Response from LLM</li> </ul> </li> </ul> </li> </ul> <ol> <li> <p>Another technique for Inference Time Scaling is to provide more data in the input instructions\u00a0\u21a9</p> </li> </ol>"},{"location":"frameworks/","title":"Frameworks","text":""},{"location":"frameworks/#langchain","title":"LangChain","text":"<ul> <li>A framework for developing GenAI apps<ul> <li>Uses <code>LangSmith</code> for LLMOps (Debugging, Evaluation, Monitoring etc.)</li> <li>Uses <code>LangServe</code> to create Chains as Rest APIs for depolyment</li> </ul> </li> </ul>"},{"location":"local/","title":"Local Execution","text":""},{"location":"local/#local-llms","title":"Local LLMs","text":""},{"location":"local/#ollama","title":"Ollama","text":""},{"location":"local/#setup","title":"Setup","text":"<ul> <li>Download Ollama</li> <li>Download model<ul> <li>Search for the desired model and go to the corresponding page<ul> <li>The <code>dolphin</code> models are uncensored (search for dolphin)</li> </ul> </li> <li>Click on the desired version, copy the cli command and execute in the terminal <ul> <li>Make sure you have enough RAM for the LLM to work </li> <li>Memory should be roughly equal to the size of the model </li> </ul> </li> <li>Once the terminal command executes we are ready to start using the LLM locally<ul> <li>When we run the 'ollama run' command for a model for the first time, it is downloaded in the <code>~/.ollama/models/manifests/registry.ollama.ai/library</code> folder in Mac</li> </ul> </li> </ul> </li> </ul> Useful Commands Command Description /bye  (or Ctrl D) quit interacting with the LLM ollama Lists all available commands ollama list Lists the models available for use ollama run  To run the model ollama serve To start a local server<sup>1</sup> <p>Also see Ollama Prompting in Python</p>"},{"location":"local/#llamafile","title":"LlamaFile","text":""},{"location":"local/#setup_1","title":"Setup","text":"<ul> <li>Download the `TinyLlama Llamfile from the Other example llamafiles section</li> <li>Grant permissions     <pre><code>chmod +x TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n</code></pre></li> <li>Start llamafile server     <pre><code>./TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n</code></pre><ul> <li>This should launch the llamafile ui on http://127.0.0.1:8080</li> </ul> </li> </ul> <p>Sample Code</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://127.0.0.1:8080/v1\",  # local llamafile url\n    api_key=\"no-key\"   # empty string does not work\n)\n\nprompt = f\"What is LLM?\"\nmessages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}]\nresponse = client.chat.completions.create(\n    model=\"TinyLLM\",\n    messages=messages,\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"local/#local-frameworks","title":"Local Frameworks","text":""},{"location":"local/#llamaindex","title":"LLamaIndex","text":""},{"location":"local/#implementation-steps","title":"Implementation Steps","text":"<ul> <li>Install required packages <pre><code># core package for LlamaIndex, which provides the framework for building and querying indexes\npip install llama-index\n# package for integrating LLaMA models with LlamaIndex, allowing for the use of LLaMA models as the language model (LLM) component\npip install llama-index-llms-llamafile\n# package for providing the embedding functionality using LLaMA models, which is essential for creating vector representations of documents\npip install llama-index-embeddings-llamafile\n# package for openai integration (required only if using openai models)\npip install llama-index-llms-openai\n</code></pre></li> <li>Set up LlamaIndex with LLaMAFile</li> <li>Define custom prompt using prompt templates (Optional)</li> <li>Load Local Data</li> <li>Build the Index</li> <li>Create a Query Engine</li> <li>Query the Index</li> </ul> <p>Sample Code</p> <pre><code>from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.embeddings.llamafile import LlamafileEmbedding\nfrom llama_index.llms.llamafile import Llamafile\n\n# configure LlamaIndex to use the LLaMAFile\n# create an embedding model that utilizes the LLaMAFile\nSettings.embed_model = LlamafileEmbedding(base_url=\"http://localhost:8080\")\n# Set up the Llama model as the LLM component of LlamaIndex\nSettings.llm = Llamafile(base_url=\"http://localhost:8080\", temperature=0, seed=0)\n\n# load local pdf docs\n# the `load_data` method loads the documents from the directory and returns them as a list. \nlocal_reader = SimpleDirectoryReader(input_dir='DirPath/')\ndocs = local_reader.load_data(show_progress=True)\n\n# create an index to store vector representations of the loaded documents. \n# the `from_documents` method builds the index from the provided list of documents.\nindex_pdf = VectorStoreIndex.from_documents(docs)\n\n# convert the index into a query engine that can handle queries and query the index\nquery_engine_pdf = index_pdf.as_query_engine()\n\n# use the query engine to retrieve relevant information from the index\nquery = \"What is the main topic of the document?\"\nresponse = query_engine.query(query)\nprint(response)\n</code></pre>"},{"location":"local/#custom-prompts","title":"Custom Prompts","text":"<p>The most commonly used prompts are <code>text_qa_template</code> (1) and <code>refine_template</code> (2).</p> <ol> <li> Used to get an initial answer to a query using retrieved nodes</li> <li> Used when the retrieved text does not fit into a single LLM call with <code>response_mode=\"compact\"</code> (the default), or when more than one node is retrieved using <code>response_mode=\"refine\"</code><ul> <li>The answer from the first query is inserted as an existing_answer, and the LLM must update or repeat the existing answer based on the new context.</li> </ul> </li> <li> This will show the ip and port of the server which can then be used as an endpoint to interact with the LLM.</li> </ol> Sample Code with Custom Prompts Completion PromptsChat Prompts <pre><code>from llama_index.core import PromptTemplate\n\ntext_qa_template_str = (\n    \"Context information is below.\\n---------------------\\n{context_str}\"\n    \"\\n---------------------\\nUsing both the context information and also using\"\n    \" your own knowledge, answer the question: {query_str}\\nIf the context isn't\"\n    \" helpful, you can also answer the question on your own.\\n\"\n)\ntext_qa_template = PromptTemplate(text_qa_template_str)\n\nrefine_template_str = (\n    \"The original question is as follows: {query_str}\\nWe have provided an existing answer: \"\n    \"{existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed)\"\n    \" with some more context below.\\n------------\\n{context_msg}\\n------------\\nUsing both the\"\n    \" new context and your own knowledge, update or repeat the existing answer.\\n\"\n)\nrefine_template = PromptTemplate(refine_template_str)\n\nprint(index_pdf.as_query_engine(\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n        ).query(\"What is the main topic of the document?\"))\n</code></pre> <pre><code>from llama_index.core.llms import ChatMessage, MessageRole\nfrom llama_index.core import ChatPromptTemplate\n\nqa_prompt_str = (\n    \"Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\n\"\n    \"Given the context information and not prior knowledge, answer the question: {query_str}\\n\"\n)\n\nrefine_prompt_str = (\n    \"We have the opportunity to refine the original answer (only if needed) with some more context below.\\n\"\n    \"------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original\"\n    \" answer to better answer the question: {query_str}. If the context isn't useful, output\"\n    \" the original answer again.\\nOriginal Answer: {existing_answer}\"\n)\n\n# Text QA Prompt\nchat_text_qa_msgs = [\n    (\"system\", \"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", qa_prompt_str),\n]\ntext_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n\n# Refine Prompt\nchat_refine_msgs = [\n    (\"system\",\"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", refine_prompt_str),\n]\nrefine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)\n\nprint(index_pdf.as_query_engine(\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n        ).query(\"What is the main topic of the document?\"))\n</code></pre>"},{"location":"local/#langchain","title":"LangChain","text":""},{"location":"local/#implementation-steps-using-llamafile","title":"Implementation Steps Using LlamaFile","text":"<ul> <li>Install required packages <pre><code>pip install -U langchain langchain-openai langchain-community\n</code></pre></li> <li>Set up Langchain with Llamafile</li> <li>Create prompt</li> <li>Invoke prompt using model</li> </ul> <p>Sample Code</p> <pre><code>from langchain_community.llms.llamafile import Llamafile\nfrom langchain.prompts import PromptTemplate\n\n# Initialize Llamafile\nllm = Llamafile(temperature=0.7)\n\n# Define prompt\nprompt_template = PromptTemplate.from_template(\"What are LLMs?\")\n\n# Invoke prompt\nprompt = prompt_template.invoke({})\nresult = llm.invoke(prompt)\n</code></pre> <ul> <li> <p>For using local files as context:</p> <ul> <li> <p>Load local data and split into manageable chunks</p> Splitters <p>CharacterTextSplitter:</p> <ul> <li>Tries to preserve paragraphs, sentences, and words as coherent units.</li> <li>Can specify chunk_size, chunk_overlap, and separator.</li> <li>Does not automatically handle very large chunks; instead, it relies on the user setting appropriate values for chunk_size and chunk_overlap.</li> </ul> <p>RecursiveCharacterTextSplitter:</p> <ul> <li>Similar to CharacterTextSplitter, but adds recursive splitting capabilities.</li> <li>Automatically handles very large chunks by attempting to split them according to the specified chunk_size and separator list.</li> <li>If a chunk remains too large after the first round of splitting, it will try again with subsequent separators in the list.</li> </ul> Loading Files <pre><code>from langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n\n# Load local data\npdf_path = 'DirPath/Filename'\npdf_loader = PyPDFLoader(file_path = pdf_path)\n\n# Split using Recursive splitter\n# split is based on the specified chunk size\nsplitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 0)\n\npdf_data_RS = pdf_loader.load_and_split(text_splitter=splitter)\n</code></pre> </li> <li> <p>Define custom prompt using prompt templates (Optional)</p> </li> <li>Instantiate chain</li> <li>Generate response by invoking chain</li> </ul> Sample Code with Custom Prompts Completion PromptsChat Prompts <pre><code>from langchain.chains.summarize import load_summarize_chain\n# create prompt\ntemplate = \"\"\"\nWrite a summary that highlights the main ideas in 3 bullet points of the following:\n\"{text}\"\nSUMMARY:\n\"\"\"\n# Create prompt\nprompt = PromptTemplate.from_template(template)\n# Instantiate chain\nchain = load_summarize_chain(\n    llm=llm,\n    chain_type='stuff',\n    prompt=prompt,\n    verbose=False   # Setting this to true will print the formatted prompt\n)\n# Invoke chain\nresults = chain.invoke(pdf_data_RS)\nprint(f\"Result Keys: {results.keys()}\")\nprint(f\"\\nOutput: {results['output_text']}\")\n</code></pre> <pre><code>from langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define prompt\nprompt = ChatPromptTemplate.from_messages(\n    [(\"system\", \"Summarize the highlights of the following in 3 bullet points:\\\\n\\\\n{context}\")]\n)\n\n# Instantiate chain\nchain = create_stuff_documents_chain(llm, prompt)\n\n# Invoke chain\nresult = chain.invoke({\"context\": pdf_data_RS[1:5]})\n</code></pre> </li> </ul> <ol> <li> <p>This will show the ip and port of the server which can then be used as an endpoint to interact with the LLM.</p> <ul> <li>If it shows the error: <code>Error: listen tcp 127.0.0.1:11434: bind: address already in use</code>, it means that ollama is already running at port 11434</li> </ul> <p>\u21a9</p> </li> </ol>"},{"location":"nlp/","title":"NLP Models","text":""},{"location":"nlp/#model-types","title":"Model Types","text":""},{"location":"nlp/#bag-of-words","title":"Bag of Words","text":"<p>Sample Code</p> <p>NTLK Corpus</p> <p>NTLK Stem</p> <p>Count Vectorizer</p> <p>Notebook</p> <pre><code>import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer   # 'extracts' the root of the words from their variations\n\n# Clean Data and Build Corpus\ncorpus = []\nps = PorterStemmer()\n\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\n\nfor i in range(0, 1000):\n    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])  # replace all punctuations by space\n    review = review.lower()    # convert to lowercase\n    review = review.split()    # split each review into list of words\n\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    review = ' '.join(review)   # join back the stemmed words for the review\n    corpus.append(review)\n\n# Build Model\nfrom sklearn.feature_extraction.text import CountVectorizer\n# max features set to a number lower than the total number of words so that it excludes the sparsely occuring words\n# total number of words can be found by checking the length of X after the fit transform step\nc_cv = CountVectorizer(max_features = 1500)  \nX = c_cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Train Model and Predict\n# This example uses Gaussian NB\n# Other classification models can also be used\nfrom sklearn.naive_bayes import GaussianNB\nc_gnb = GaussianNB()\nc_gnb.fit(X_train, y_train)\ny_pred_gnb = c_gnb.predict(X_test)\n</code></pre>"},{"location":"nlp/#latent-dirichlet-allocation-lda","title":"Latent Dirichlet Allocation (LDA)","text":"<p>Sample Code (1)</p> <pre><code>from sklearn.decomposition import LatentDirichletAllocation\n\n# Define X and y.\nX = df_yelp.review\ny = df_yelp.stars\n\n# Split the new DataFrame into training and testing sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25 ,random_state=13)\n\n# Use CountVectorizer to create document-term matrices from X_train and X_test.\nvect = CountVectorizer(lowercase=False, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\nX_train_dtm = vect.fit_transform(X_train)\n\nnumber_of_topics = 10\n\nmodel = LatentDirichletAllocation(n_components=number_of_topics, random_state=13)\nmodel.fit(X_train_dtm)\n\nno_top_words = 10\ntopic_dict = {}\nfor topic_idx, topic in enumerate(model.components_):\n    topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(vect.get_feature_names_out()[i])\n                    for i in topic.argsort()[:-no_top_words - 1:-1]]\n    topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n                    for i in topic.argsort()[:-no_top_words - 1:-1]]\n\ntopic_df = pd.DataFrame(topic_dict)\n</code></pre> <ol> <li> <p>LatentDirichletAllocation</p> <p>Notebook</p> </li> </ol>"},{"location":"nlp/#bertopic","title":"BERTopic","text":"<p>Sample Code</p> <p>BERTopic</p> <p>Notebook</p> <pre><code>from bertopic import BERTopic\n\nnlp = spacy.load('en_core_web_sm', exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n\ntopic_model = BERTopic(embedding_model=nlp)\ntopics, probs = topic_model.fit_transform(df_yelp['review'])\n\ntopic_model.get_topic_info()\n</code></pre>"},{"location":"nlpb/","title":"NLP Modeling Basics","text":"Useful Libraries and Packages <ul> <li> <p>Natural Language Toolkit (NLTK) - NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries</p> <ul> <li>Punkt - In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data. </li> </ul> </li> <li> <p>TextBlob - This library provides a simplified interface for exploring common NLP tasks including part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation etc.</p> </li> <li> <p>spaCy - spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It helps build applications that process and \u201cunderstand\u201d large volumes of text. It can be used to build information extraction or natural language understanding systems.</p> </li> <li> <p>Gensim - It is one of the fastest library for training of vector embeddings . The core algorithms in Gensim use battle-hardened, highly optimized &amp; parallelized C routines. Gensim can process arbitrarily large corpora, using data-streamed algorithms. There are no \"dataset must fit in RAM\" limitations.</p> </li> <li> <p>Universal Dependencies - Universal Dependencies (UD) is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages.</p> </li> <li> <p>BERTopic - BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions</p> </li> </ul>"},{"location":"nlpb/#modeling-steps","title":"Modeling Steps","text":"<ul> <li>Generate or Read data</li> <li>Preprocess and clean data<ul> <li>Remove stopwords</li> <li>Remove links</li> <li>Remove punctuations; keep only alpha characters</li> <li>Remove double spacing</li> <li>Extract word root</li> <li>Convert to lowercase: One common method of reducing the number of features is converting all text to lowercase before generating features <ul> <li>It might be useful not to convert them to lowercase if capitalization matters.</li> </ul> </li> </ul> </li> <li>Split training and test data</li> <li>Vectorize Data</li> <li>Apply ML Classifier<ul> <li>Model Training (<code>model.fit</code>)</li> </ul> </li> <li>Get processing output</li> </ul> <p>Also see General ML Modeling Steps</p>"},{"location":"nlpb/#tokenization","title":"Tokenization","text":""},{"location":"nlpb/#with-bert","title":"With BERT","text":"<p>The BERT tokenizer has an <code>encode</code>function similar to <code>convert_tokens_to_ids</code> that includes special tokens such as (beginning of the sequence) and (end of the sequence). These special tokens help the model understand where a sequence starts and ends. <p>Sample Code</p> <pre><code>from transformers import BertTokenizer\n\n# Initialize pre trained tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",clean_up_tokenization_spaces=True)\n# Tokenize sentence\ntokens = tokenizer.tokenize(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# convert the tokens to their corresponding numerical values\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n# use encode function to convert the tokens to their corresponding numerical values\nenc_token_ids = tokenizer.encode(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# convert ids back to tokens to inspect the special token ids added by the encode function\ndec_tokens = tokenizer.convert_ids_to_tokens(enc_token_ids)\n</code></pre>"},{"location":"nlpb/#with-spacy","title":"With spaCy","text":"<p>On parsing, the following components are identified for each token:</p> <ul> <li><code>Text</code>: The original word text.</li> <li><code>Lemma</code>: The base form of the word.</li> <li><code>POS</code>: The simple UPOS part-of-speech tag.</li> <li><code>Tag</code>: The detailed part-of-speech tag.</li> <li><code>Dep</code>: Syntactic dependency, i.e. the relation between tokens.</li> <li><code>Shape</code>: The word shape \u2013 capitalization, punctuation, digits.</li> <li><code>is alpha</code>: Is the token an alpha character?</li> <li><code>is stop</code>: Is the token part of a stop list, i.e. the most common words of the language?</li> </ul> <p>Also see spaCy Linguistic Annotations</p> <p>Sample Code</p> <pre><code>import spacy\nfrom spacy import displacy   # for vizualizing dependencies\n\n# Load corpus\nnlp = spacy.load(\"en_core_web_sm\")\n# Tokenize text using above corpus\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# Vizualize dependencies\nfrom spacy import displacy\nhtml = displacy.render(doc, style=\"dep\")\n\n# Print parsed building blocks\nfor ix, token in enumerate(doc):\nprint(ix, token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n</code></pre>"},{"location":"nlpb/#with-tittoken","title":"With TitToken","text":"<p>Sample Code</p> <pre><code>import tiktoken\n\nencoding = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n\ntokens = encoding.encode(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor token_id in tokens:\n    token_text = encoding.decode([token_id])\n    print(f\"{token_id} = {token_text}\")\n</code></pre>"},{"location":"nlpb/#spacy-ner","title":"spaCy NER","text":"<p>spaCy can recognize various types of named entities in a document and show the label of the named entity as well as the position of the entity in the document. spaCy NER returns the following:</p> <ul> <li><code>Text</code>: The original entity text.</li> <li><code>Start</code>: Index of start of entity in the Doc.</li> <li><code>End</code>: Index of end of entity in the Doc.</li> <li><code>Label</code>: Entity label, i.e. type.</li> </ul> <p>Sample Code</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# Parse entities using .ents\nfor ix, ent in enumerate(doc.ents):\n    print(ix, ent.text, ent.start_char, ent.end_char, ent.label_)\n</code></pre>"},{"location":"nlpb/#spacy-pipelines","title":"spaCy Pipelines","text":"<p>spaCy uses a specific component such as <code>Tagger</code>, <code>DependencyParser</code>, <code>EntityRecognizer</code>, <code>Lemmatizer</code>, <code>TextCategorizer</code> etc. for these pipelines. Pipeline componets can be added and retrieved using the add_pipe and get_pipe methods respectively. </p> <ul> <li>One of the componets called the AttributeRuler can be used to handle exceptions for token attributes and to map values between attributes such as mapping fine-grained POS tags to coarse-grained POS tags.</li> </ul> <p>Also see spaCy Pipeline documentation and spaCy Processing Pipeline documentation. </p> <p>Sample Code</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I saw The Beatles perform. Who did you see?\")\n\n# Get pipe for lemmatizer\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\nprint(lemmatizer.mode)\n\n# Get pipe for attribute ruler and modify\nruler = nlp.get_pipe(\"attribute_ruler\")\n# Pattern to match \"The Beatles\"\npatterns = [[{\"LOWER\": \"the\"}, {\"TEXT\": \"Beatles\"}]]\n# The attributes to assign to the matched token\nattrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n# Add rules to the attribute ruler\nruler.add(patterns=patterns, attrs=attrs, index=0)  # \"The\" in \"The Beatles\"\nruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Beatles\" in \"The Beatles\"\n</code></pre>"},{"location":"nlpb/#vectorization","title":"Vectorization","text":"<p>Sample Code</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\nc_cv = CountVectorizer()\nX = c_cv.fit_transform(df_yelp.text)\n\n# Last 10 features\nprint((c_cv.get_feature_names_out()[-10:]))\n\n# print first 10 items in the vocab\nfrom itertools import islice\nlist(islice(c_cv.vocabulary_.items(), 10))\n</code></pre>"},{"location":"nlpb/#embeddings","title":"Embeddings","text":"<p>Sample Code</p> <pre><code>import torch\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# get the embedding vector for the word \"apple\"\ntoken_id_apple = tokenizer.convert_tokens_to_ids([\"apple\"])[0]\nembedding_apple = model.embeddings.word_embeddings(torch.tensor([token_id_apple]))\n\n# get the embedding vector for the word \"orange\"\ntoken_id_orange = tokenizer.convert_tokens_to_ids([\"orange\"])[0]\nembedding_orange = model.embeddings.word_embeddings(torch.tensor([token_id_orange]))\n\n# Check similarity between two words based on their embeddings\ncos = torch.nn.CosineSimilarity(dim=1)\nsimilarity = cos(embedding_apple, embedding_orange)\nprint(f\"Cosine Similarity between 'apple' and 'orange': {similarity[0] * 100}%\")\n</code></pre>"},{"location":"prompt-code/","title":"Code Snippets","text":""},{"location":"prompt-code/#openai","title":"OpenAI","text":"<p>Sample Code</p> <pre><code>import os\nfrom dotenv import load_dotenv, find_dotenv\nfrom openai import OpenAI\n\nmodel=\"gpt-5-nano\"\nopenai = OpenAI()  # Automatically uses the env variable OPENAI_API_KEY. \n\n# We can also specify the api_key parameter explicitly\n_ = load_dotenv(find_dotenv())\n# The following can also be used\n# load_dotenv(override=True)\napi_key  = os.getenv('OPENAI_API_KEY')\nopenai = OpenAI(api_key=api_key)\n\nsystem_prompt = \"You are an useful assistant.\"\nuser_prompt_prefix = \"Tell me a joke.\"\n\n# Helper function for message\ndef get_message(user_prompt):\nreturn [{\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt}]\n\n# Helper function for prompting\ndef get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=200):\n    response = openai.chat.completions.create(\n        model=model,\n        messages=get_message(prompt),\n        max_tokens=max_tokens,\n        temperature=temperature, \n    )\n    return response.choices[0].message.content\n\nprompt = user_prompt_prefix + \" It should be about movies.\"\n\nresponse = get_completion(prompt)\nprint(response)\n</code></pre> Using POST <pre><code># The following represents the POST equivalent of the code chunk showin the Usage section above\nimport os\nimport requests\n\napi_key  = os.getenv('OPENAI_API_KEY')\nheaders = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\npayload = {\n    \"model\": \"gpt-5-nano\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are an useful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a joke.\"}]\n}\n\nresponse = requests.post(\n    \"https://api.openai.com/v1/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\nprompt_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\nprint(prompt_response)\n</code></pre>"},{"location":"prompt-code/#google","title":"Google","text":"<p>Sample Code</p> <pre><code>from openai import OpenAI\n\nGEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\n\ngemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n\nresponse = gemini.chat.completions.create(\n            model=\"gemini-2.5-flash-lite\", \n            messages=get_message(prompt))\n\nresponse.choices[0].message.content\n</code></pre>"},{"location":"prompt-code/#ollama","title":"Ollama","text":"<p>Ensure that Ollama is running before using this code.</p> <p>Sample Code</p> <pre><code># Check if Ollama is running\n# The statement below should return \"Ollama is running\"\nrequests.get(\"http://localhost:11434\").content\n\nOLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n\nollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='dummy')  # api_key just needs a dummy value\n\nresponse = ollama.chat.completions.create(\n            model=\"llama3.1:8b\", \n            messages=get_message(prompt))\n\nresponse.choices[0].message.content\n</code></pre>"},{"location":"prompt-samples/","title":"Samples","text":""},{"location":"prompt-samples/#general","title":"General<sup>1</sup>","text":""},{"location":"prompt-samples/#summarizing","title":"Summarizing","text":"<p>Example 1 - Your task is to generate a short summary of a product review to give feedback to the pricing department. Summarize the review below, delimited by triple backticks, in at most 2 sentences, and focusing on any aspects that are relevant to the price and perceived value. <code>```{text}```</code></p> <p>Example 2 (Use 'extract' instead of summarize) - Your task is to generate a short summary of a product review to give feedback to the pricing deparmtment. From the review below, delimited by triple quotes extract the information relevant to the price and perceived value. Limit to 2 sentences. <code>```{text}```</code></p>"},{"location":"prompt-samples/#inferring","title":"Inferring","text":""},{"location":"prompt-samples/#sentiments-and-emotions","title":"Sentiments and Emotions","text":"<p>Example 1 - What is the sentiment of the writer in the following text, which is delimited with triple backticks? Give your answer as a single word, either \"positive\" or \"negative\". <code>```{text}```</code></p> <p>Example 2 - Identify a list of emotions expressed by the writer of the following review . Include no more than three items in the list.  Format your answer as a comma separated list in upper case. <code>```{text}```</code></p>"},{"location":"prompt-samples/#specific-pieces-of-information","title":"Specific Pieces of Information","text":"<p>Identify the following items from the text delimited with triple backticks: - Movie Name - Year Of Release Format your response as a JSON object with \"Name\" and \"Year\" as the keys.  If the information isn't present, use \"unknown\" as the value. <code>```{text}```</code></p>"},{"location":"prompt-samples/#topics","title":"Topics","text":"<p>Example 1 - Determine five topics that are being discussed in the following text, which is delimited by triple backticks. Make each item one or two words long. Format your response as a list of items separated by commas. <code>```{text}```</code></p> <p>Example 2 (Zero Shot Algorithm) - Determine whether each item in the following list of topics is a topic in the text below, which is delimited with triple backticks. Give your answer as list with 0 or 1 for each topic. Format the response as a JSON  with the following keys: topic, included List of topics: `python topic_list = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"] {\", \".join(topic_list)} ` <code>```{text}```</code></p>"},{"location":"prompt-samples/#transforming","title":"Transforming","text":""},{"location":"prompt-samples/#language-translation","title":"Language Translation","text":"<p>Translate the following text delimited by triple backticks to bengali. Provide both formal and informal translations. <code>```Who are you```</code></p>"},{"location":"prompt-samples/#tone-transformation","title":"Tone Transformation","text":"<p>Translate the following from slang to a business letter: 'Holy shit, This stuff doesn't work. Get me a new one.'</p>"},{"location":"prompt-samples/#format-conversion","title":"Format Conversion","text":"<p>Translate the following python dictionary from JSON to an HTML table with column headers and title: {json_text}</p>"},{"location":"prompt-samples/#grammer-check","title":"Grammer Check","text":"<p>Proofread and correct the following text and rewrite the corrected version. Make it more compelling. <code>```{text}```</code></p>"},{"location":"prompt-samples/#email-response","title":"Email Response","text":"<p>Your task is to send an email reply to a valued customer. Given the customer email delimited by ```, identify the sentiment of the email. Generate a reply to thank the customer for their review. If the sentiment is positive or neutral, thank them for their review. If the sentiment is negative, apologize and suggest that they can reach out to customer service. Make sure to use specific details from the review. Write in a concise and professional tone. Sign the email as <code>AI customer agent</code>. <code>```{email_text}```</code></p>"},{"location":"prompt-samples/#coding","title":"Coding","text":""},{"location":"prompt-samples/#python","title":"Python","text":""},{"location":"prompt-samples/#get-data","title":"Get Data","text":"<p>You are an expert in python and data analysis in pandas, numpy and matplotlib. We will be working with google collab for writing code. The project for today is to write the code to make a few plots. First we need to extract the data frame of passengers on titanic from the following link. https://en.wikipedia.org/wiki/Passengers_of_the_Titanic Give me the code to do that.  </p>"},{"location":"prompt-samples/#create-visuals","title":"Create Visuals","text":"<p>Let's create a histogram using the matplotlib library, ensuring that xlabels and ticks don't overlap and the graph is visually appealing for a PowerPoint presentation.</p>"},{"location":"prompt-samples/#generate-content","title":"Generate Content","text":"<p>Run a python script to generate a pptx file containing the above visuals. Include 2 bullet points in each slide to summarize the insights from the visuals.</p> <ol> <li> <p>Based on DeepLearning.ai \u21a9</p> </li> </ol>"},{"location":"prompts/","title":"Overview","text":""},{"location":"prompts/#types","title":"Types","text":""},{"location":"prompts/#completion-prompts","title":"Completion Prompts","text":"<ul> <li>Suitable for single-turn tasks where the model generates a response based on a single input prompt</li> <li>Conversation context/history and role seggregation is not essential</li> <li>Works well for content generation, summarization, question-answering etc.</li> </ul>"},{"location":"prompts/#chat-prompts","title":"Chat Prompts","text":"<ul> <li>Designed for multi-turn conversations with multiple roles (user, assistant, system etc.)<ul> <li>System prompt - indicates what task the AI is performing and what tone it should use</li> <li>User prompt - the prompt from user that the AI should reply to</li> </ul> </li> <li>Maintains conversation context by processing the entire conversation history</li> <li>Ideal for chatbot applications and tasks requiring back-and-forth interactions</li> </ul>"},{"location":"prompts/#prompting-guidelines","title":"Prompting Guidelines","text":""},{"location":"prompts/#structure","title":"Structure","text":"<ul> <li> <p>Break down large prompts into steps</p> <ul> <li> <p>Use delimiters for organizing the prompt</p> Stop Sequences <ul> <li>Can be used to signal the end of an example in few-shot prompting </li> <li>Helps to structure the input data for the model</li> <li>Triple backticks, quotes, double hashtags etc.</li> </ul> </li> <li> <p>Break large prompts into \"sub-prompts\" if needed</p> </li> <li>Context: Brief introduction or background information</li> <li>Include topic, industry/field, relevant links, keywords, datasets, text chunks etc.</li> <li>Constraints or restrictions</li> <li>Persona: Role or expertise/profession the prompt should use</li> <li>Use phrases such as <code>Act as</code>, <code>Your role is to</code>, <code>Imagine you are</code> etc.</li> <li>Add relevant details such as field of work, company name, time period, region etc.</li> <li>Goal/Intent: Define what the prompt should do</li> <li>Describe objective<ul> <li>Use action verbs such as <code>Analyze</code>, <code>Generate</code>, <code>Simplify</code>, <code>Summarize</code> etc.</li> </ul> </li> <li>Mention any focus areas using phrases such as <code>Focus on</code> etc.</li> </ul> Useful Action Verbs and Adjectives <ul> <li>Verbs<ul> <li>Analyze, Compare, Convert, Customize, Describe, Evaluate, Explain, Generate, Improve, Optimize, Organize, Proofread, Provide, Revise, Rewrite, Share, Simplify, Translate, Write</li> </ul> </li> <li>Adjectives<ul> <li>Clear, Concise, Conversational, Professional</li> </ul> </li> </ul> </li> <li> <p>Output</p> <ul> <li>Audience: Layman, x year old, type of professional, demographics (age/gender/location) etc.</li> <li>Tone: Formal, Business, Casual, Academic, Humorous etc.<ul> <li>As a Famous Personality e.g. Shakespeare, Tagore etc.</li> </ul> </li> <li>Other Specs: <ul> <li>Specify constraints (Number of words, points, lines, paragraphs etc.): <ul> <li>Summarize using x bullet points, numbered lists etc.</li> <li>Make your response as short as possible.</li> </ul> </li> <li>Specify length of output<ul> <li>Use at most x words.</li> <li>Use no more than x sentences.</li> <li>Give your answer as a single word, either \"this\" or \"that\".</li> </ul> </li> <li>Ask for additional details <ul> <li>examples and/or analogies</li> <li>for and against arguments</li> </ul> </li> </ul> </li> </ul> </li> <li>Clarify: Ask the prompt to ask clarification questions if needed<ul> <li>Ask me questions (one at a time) to (add relevant text based on the ask)</li> </ul> </li> <li>Validate: Ask for proff for reducing hallucinations  and fabricated responses<ul> <li>Ask model to find relevant information, then answer the question based on the relevant information</li> <li>Provide relevant sources and citations</li> </ul> </li> </ul> Code chunks in Prompts <p>Include code chunks in a prompt by enclosing them in single backticks followed by the name of the language: <pre><code>Some text\n`python\nsome python code\n`\nMore text\n</code></pre></p>"},{"location":"prompts/#tactics","title":"Tactics<sup>1</sup>","text":"<ul> <li> <p>Tactic 1: Use delimiters</p> <ul> <li>Triple Quotes: <code>\"\"\"</code></li> <li>Triple backticks: <code>```</code></li> <li>Triple dashes: <code>---</code></li> <li>Angle brackets: <code>&lt;&gt;</code></li> <li>XML tags: <code>\\&lt;tag&gt;\\&lt;/tag&gt;</code></li> </ul> Sample <pre><code>text = f\"\"\"\nA paragraph of text\n\"\"\"\n\nprompt = f\"\"\"\nSummarize the text delimited by triple backticks into a single sentence.\n```{text}``` \n\"\"\"\n</code></pre> </li> <li> <p>Tactic 2: Ask for a structured output in a specified format</p> <ul> <li>JSON, HTML etc.</li> <li>Markdown with structured sections and headings</li> </ul> Sample <pre><code># Example 1 - Simple JSON with specified keys\nprompt = f\"\"\"\nGenerate a list of three bollywood movies along with their director and year of release. \n\nProvide them in JSON format with the following keys: \nMovie Name, Year of Release, Director.\n\"\"\"\n</code></pre> <pre><code># Example 2 - Structured JSON\nprompt = f\"\"\"\nYour task is to perform the following actions: \n1 - Write a parable.\n2 - Summarize it in a single line in Bangla.\n3 - State the moral of the parable.\n4 - Output a json object that contains the following keys: title, moral.\n\nUse the following format:\nParable: &lt;text to summarize&gt;\nSummary: &lt;summary&gt;\nMoral: &lt;moral of story&gt;\nOutput JSON: &lt;json with title and moral&gt;\n\"\"\"\n</code></pre> <pre><code># Example 3 - HTML Table with specified columns and datatype\nprompt = f\"\"\"\nYour task is to perform the following actions: \n1. Prepare a list of 10 bollywood movies\n2. Find out year each of the movies was released\n3. Find out if the movie was a hit or a flop at the box office.\n\nOutput the response as a table having three columns.\nIn the first column include the name of the movie.  Use \"Name\" as the column title.\nIn the second column include the year of release (Use \"Year\" as the column title ) and in the third column, include the box office result. Use \"Hit or Flop\" as the column title and format the value as a boolean.\n\nGive the table the title 'Movie Stats'.\n\nFormat everything as HTML that can be used in a website. \nPlace the description in a &lt;div&gt; element.\n\nOutput in markdown format as well. \n\"\"\"\n</code></pre> <pre><code># Example 4 - Markdown\nprompt = f\"\"\"\nRespond to the following in markdown format. Do not wrap the markdown in a code block - respond just with the markdown: \n```{text}```\n\"\"\"\n</code></pre> </li> <li> <p>Tactic 3: Ask the model to check whether conditions are satisfied</p> Sample <pre><code>text = f\"\"\"\nx=1 y=2 x+y=3\n\"\"\"\n\nprompt = f\"\"\"\nYou will be provided with text delimited by triple quotes. \nIf it contains mathematical equations, re-write the equation as a list of steps in the following format:\n\nStep 1 - ...\nStep 2 - \u2026\n\u2026\nStep N - \u2026\n\nIf the text does not contain contains a mathematical equation, then simply write \\\"No equation provided.\\\"\n\n\\\"\\\"\\\"{text}\\\"\\\"\\\"\n\"\"\"\n</code></pre> </li> </ul> <p></p> <ul> <li> <p>Tactic 4 - Few-Shot Prompting</p> <ul> <li>Provide examples of completing tasks. Then ask model to perform the task</li> </ul> Sample <pre><code>prompt = f\"\"\"\nYour task is to answer in a consistent style.\n\n&lt;question&gt;: Sholay.\n\n&lt;answer&gt;: The movie \"Sholay\" was released in the 70s and started Amitabh and Dharmendra in the lead roles.\n\n&lt;question&gt;: Karz.\n\"\"\"\n</code></pre> </li> <li> <p>Tactic 5: Specify the steps required to complete a task</p> Sample <pre><code>prompt = f\"\"\"\nPerform the following actions: \n1 - Write a parable.\n2 - Summarize it in a single line in Hindi.\n3 - State the moral of the parable.\n4 - Output a json object that contains the following keys: title, moral.\n\nSeparate your answers with line breaks.\n\"\"\"\n</code></pre> </li> <li> <p>Tactic 6: Instruct the model to work out its own solution before rushing to a conclusion</p> Sample <pre><code>prompt = f\"\"\"\nYour task is to determine if the student's solution is correct or not.\nTo solve the problem do the following:\n- First, work out your own solution to the problem. \n- Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\n\nUse the following format:\nQuestion:\n- question here\nStudent's solution:\n- student's solution here\nActual solution:\n- steps to work out the solution and your solution here\nIs the student's solution the same as actual solution just calculated:\n- yes or no\nStudent grade:\n- correct or incorrect\n\nQuestion:\nI'm producing a television show and need help\nworking out the cost associated to the payments for the actors. \n- Lead actor charges $100 / episode\n- Lead actress charges $90 / episode\n- I negotiated a contract for character actors that will cost me a flat $1K for the first 10 character actors for the entire show, and an additional $25 per additional character actor / episode\nWhat is the total amount I need to spend on the actors as a function of the number of episodes. Assume that I end up using a total of 12 character actors per episode.\n\nStudent's solution:\nLet x be the number of episodes.\nCosts:\n1. Lead actor charges: 100x\n2. Lead actress charges: 90x\n3. Character actor charges: 1,000 + 300x\nTotal cost: 100x + 90x + 1,000 + 300x = 490x + 1,000\n\nActual solution:\n\"\"\"\n</code></pre> </li> </ul>"},{"location":"prompts/#basic-examples","title":"Basic Examples","text":"<ul> <li>Provide context<ul> <li>Here is some information to include in /for reference</li> </ul> </li> <li>Summarize<ul> <li>Web Pages: Paste the link of the web page followed by the word <code>summarize</code></li> <li>Write a summary of the book xyz by abc</li> <li>Provide an overview of some event</li> </ul> </li> <li>Compare<ul> <li>Compare and contrast the concepts of (or the words ..) a and b</li> </ul> </li> <li>Explain/Describe <ul> <li>Explain the concept of xyz in simple terms</li> <li>Explain the concept of xyz using abc analogy</li> <li>Describe the top x abc</li> </ul> </li> <li>Generate<ul> <li>Write a simple recipe for xyz</li> <li>Share x tips for abc</li> <li>Help me brainstorm xyz</li> </ul> </li> <li>Translate/Convert<ul> <li>Translate the following from one language to another language</li> <li>Convert the following to xyz format</li> </ul> </li> <li>Proofread/Review<ul> <li>Proofread and correct any errors in the following</li> <li>Rephrase/Structure the above for maximum impact</li> <li>Review the above and suggest areas for improvement in terms of tone and engagement</li> <li>Is there something I should add to make this resonate better?</li> </ul> </li> <li>Follow up Instructions: Additional prompts to make changes to initial output<ul> <li>Provide x alternate versions for </li> <li>Make the content more/less xyz</li> <li>Incorporate xyz to make the output more abc</li> <li>Add more details / expand on xyz </li> <li>Focus on xyz</li> </ul> </li> </ul> <p>See Prompt Samples for more examples</p> <ol> <li> <p>Based on DeepLearning.ai \u21a9</p> </li> </ol>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NLP Concepts","text":"Terminology <ul> <li>Corpus: A large collection of text documents or spoken language data used for training and testing NLP models</li> <li>Dimensionality: Number of dimensions in a word vector. A high-dimensional word vector would have many dimensions, allowing it to capture a wide range of characteristics and nuances of the word.</li> <li>Embedding: Vectorization in multi dimensional space. To give tokens meaning, the model must be trained on them. This allows the model to learn the meanings of words and how they relate to other words. To achieve this, the word vectors are \u201cembedded\u201d into an embedding space.</li> <li>Named Entity Recognition(NER): A named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. Also see spaCy NER.</li> <li>Pipelines: The steps used for processing a document and extract the constituent building blocks. Also see spaCy Pipelines</li> <li>Stemming / Lemmatization: Reducing words to their base or root form to handle variations of the same word, such as singular/plural forms or verb tenses</li> <li>Stopwords: Commonly used words (determinants, conjunctions, prepositions, pronouns, auxillary verbs etc.) in a language but do not carry much meaning or significance. </li> <li>Tokenization: The process of parsing a statement and representing them as the constituent words or punctuations.  For example, dividing text by whitespace is one common approach, but there are many others. The purpose of tokenization is to create a vocabulary from a corpus. Each token is assigned a unique id to represent it as a number. Also see spaCy Tokenization</li> <li>Transformers: Deep learning models that are trained on vectors to understand the meaning of words and how they relate to each other.</li> <li>Vector / Word Vector: Vector representation of a word in NLP. Each word vector can be thought of as a point in a multi-dimensional space, where each dimension represents a particular aspect or characteristic of the word. Combining all the dimensions in a vector allows the model to understand the word\u2019s meaning and how it relates to other words.</li> <li>Vectorization: Machine learning models typically require the data/features to be numeric. The process of transforming non-numeric data such as tokens into numeric features is called vectorization. During vectorization, the unique ids in the tokens are assigned to randomly initialized n-dimensional vectors.</li> <li>Vocabulary: Set of unique tokens found within the corpus</li> </ul>"},{"location":"#building-blocks","title":"Building Blocks","text":"<ul> <li>Tokenization: Breaking text into tokens (words, sentences, n-grams)</li> <li>Stop-word removal: a/an/the</li> <li>Stemming and lemmatization: root word and word relation detection</li> <li>TF-IDF (Term Frequency Inverse Document Frequency): word importance/uniqueness with regards to a given document</li> <li>Part-of-speech tagging: noun/verb/adjective</li> <li>Noun Phrase Extraction:</li> <li>NER (Named Entity Recognition): person/organization/location</li> <li>Spelling correction: \"Santa Calra Country\"</li> <li>Word sense disambiguation: \"Apple stock price\"</li> <li>Segmentation: \"San Francisco City subway\"</li> <li>Language detection: \"translate this page\"</li> <li>Sentiment Analysis:</li> </ul>"},{"location":"#tokenization","title":"Tokenization","text":"<ul> <li>Character tokenization - Treats each character as a separate token</li> <li>Word tokenization - Divides the text into individual words or even subwords</li> <li>Subword tokenization - Combines the benefits of character and word tokenization by breaking down rare words into smaller units while keeping frequent words as unique entities.<ul> <li>Allows the model to handle complex words and misspellings while keeping the length of the inputs manageable.</li> </ul> </li> <li>Sentence tokenization - Divides the text into individual sentences</li> </ul>"},{"location":"#vectorization","title":"Vectorization","text":""},{"location":"#bag-of-words","title":"Bag Of Words","text":"<p>This is the simplest vectorization technique and involves three operations:</p> <ul> <li>Tokenization: The input is represented as a list of its constituent words, disregarding grammar and word order.<ul> <li>It is called <code>bag of words</code> because the document's structure is lost \u2014 as if the words are all jumbled up in a bag.</li> </ul> </li> <li>Vocabulary creation: Of all the obtained tokenized words, only unique words are selected to create the vocabulary.<ul> <li>CountVectorizer creates a dictionary called <code>vocabulary_</code> that converts each word to its index in the sparse matrix.</li> </ul> </li> <li>Vector creation: A sparse matrix is created out of the frequency of vocabulary words. In this sparse matrix, each row is a sentence vector whose length (the columns of the matrix) is equal to the size of the vocabulary.<ul> <li>The words are the features(columns) and the feature vectorization just involves marking the feature as a 0 or 1 depending on whether it appears in the document or not</li> <li>Another approach is to count the number of times each word (feature) appears in the document.</li> <li>The vectorization will be mostly zeros, since only a few unique words are in each document.<ul> <li>For that reason, instead of storing all the zeros we only store non-zero values inside the 'sparse matrix' data structure.</li> </ul> </li> </ul> </li> </ul>"},{"location":"#common-vectorization-options","title":"Common Vectorization Options","text":""},{"location":"#n-grams","title":"N-Grams","text":"<p>Allows us to specify whether a group of words should be considered as a single feature</p> <ul> <li>We can specify a lower and upper boundary of the range of n-values for different n-grams to be extracted</li> <li>Adds all word combinations as specified by the lower and upper boundaries<ul> <li>For example, lower=1, upper=2 will include al single and double word combinations (multiple word combinations are only for consecutive words)</li> </ul> </li> <li>When we produce n-grams from a document with  \ud835\udc4a  words, we add a maximum of \\(\ud835\udc5b\u2212\ud835\udc4a+1\\) features. </li> <li>May lead to <code>feature explosion</code></li> </ul>"},{"location":"#topic-modeling","title":"Topic Modeling","text":"<p>A frequently used approach to discover hidden semantic patterns portrayed by a text corpus and automatically identify topics that exist inside it.</p> <ul> <li>A type of statistical modeling that leverages <code>unsupervised machine learning</code> to analyze and identify clusters or groups of similar words within a body of text.</li> <li>Topics are the latent descriptions of a corpus of text<ul> <li>Documents related to a specific topic are more likely to produce certain words more frequently</li> </ul> </li> <li>Two popular topic modeling techniques:  Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)</li> </ul>"},{"location":"#latent-semantic-analysis-lsa","title":"Latent Semantic Analysis (LSA)","text":"<p>A natural language processing technique used to analyze relationships between documents and the terms they contain. </p> <ul> <li>Assumes words with similar meanings will appear in similar documents. </li> <li>Constructs a matrix containing the word counts per document, where each row represents a unique word, and columns represent each document</li> <li>Uses Singular Value Decomposition (SVD) to reduce the number of rows while preserving the similarity structure among columns</li> <li>Cosine similarity is used to determine the similarity between documents<ul> <li>A value close to 1 means the documents are very similar based on the words in them</li> <li>A value close to 0 means they're quite different</li> </ul> </li> </ul>"},{"location":"#latent-dirichlet-allocation-lda","title":"Latent Dirichlet Allocation (LDA)","text":"<p>A natural language technique used to identify topics a document belongs to based on the words contained within it.</p> <ul> <li>It is a Bayesian network i.e., it\u2019s a generative statistical model that assumes documents are made up of words that aid in determining the topics</li> <li>Documents are mapped to a list of topics by assigning each word in the document to different topics</li> <li>This model ignores the order of words occurring in a document and treats them as a bag of words</li> </ul>"},{"location":"genai/","title":"GenAI Basics","text":""},{"location":"genai/#prompts","title":"Prompts","text":""},{"location":"genai/#completion-prompts","title":"Completion Prompts","text":"<ul> <li>Suitable for single-turn tasks where the model generates a response based on a single input prompt</li> <li>Conversation context/history and role seggregation is not essential</li> <li>Works well for content generation, summarization, question-answering etc.</li> </ul>"},{"location":"genai/#chat-prompts","title":"Chat Prompts","text":"<ul> <li>Designed for multi-turn conversations with multiple roles (user, assistant, system etc.)</li> <li>Maintains conversation context by processing the entire conversation history</li> <li>Ideal for chatbot applications and tasks requiring back-and-forth interactions</li> </ul>"},{"location":"genai/#openai","title":"OpenAI","text":"<p>API Parameters</p> <ul> <li>Max Tokens: Determines the maximum number of tokens that can be generated. This parameter helps control the verbosity of the response. The value typically ranges between 0 and 2048, though it can vary depending on the model and context.</li> <li>Temperature: Controls the randomness of the output. It influences how creative (less predictive responses) or deterministic (more predictive responses) the responses are. The value ranges between 0 and 2. The default value is 0.8.</li> <li>Top P (Nucleus Sampling): Dictates the variety in responses by only considering the top \u2018P\u2019 percent of probable words. It is an alternative to sampling with temperature and controls the diversity of the generated text. The value ranges between 0 and 1. The default value is 0.95.</li> <li>Frequency Penalty: Reduces repetition by decreasing the likelihood of frequently used words. It penalizes new tokens based on their existing frequency in the text so far. The value ranges between -2.0 and 2.0. This setting is disabled by default (value 0)</li> <li>Presence Penalty: Promotes the introduction of new topics in the conversation. It penalizes new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. The value ranges between -2.0 and 2.0. Positive values encourage diverse ideas and minimize repetition. This setting is disabled by default (value 0)</li> </ul> Temperature and Top P sample values Use Case Temperature Top P Description Chatbot 0.5 0.5 Generates responses that balance coherence and diversity resulting in a more natural and engaging conversational tone. Code Generation 0.2 0.1 Generates code that adheres to established patterns and conventions. Code output is more focussed and syntactically correct. Code Comment Generation 0.3 0.2 Generates concise and relevant code comments that adhere to conventions. Creative Writing 0.7 0.8 Generates creative and diverse text less constrained by patterns. Data Analysis Scripting 0.2 0.1 Generates focussed, efficient and correct analysis scripts. Exploratory Code Writing 0.6 0.7 Generates creative code that considers and explores multiple solutions."},{"location":"local/","title":"Local Execution","text":""},{"location":"local/#local-llms","title":"Local LLMs","text":""},{"location":"local/#llamafile","title":"LlamaFile","text":""},{"location":"local/#setup","title":"Setup","text":"<ul> <li>Download the `TinyLlama Llamfile from the Other example llamafiles section</li> <li>Grant permissions     <pre><code>chmod +x TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n</code></pre></li> <li>Start llamafile server     <pre><code>./TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n</code></pre><ul> <li>This should launch the llamafile ui on http://127.0.0.1:8080</li> </ul> </li> </ul> <p>Sample Code</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://127.0.0.1:8080/v1\",  # local llamafile url\n    api_key=\"no-key\"   # empty string does not work\n)\n\nprompt = f\"What is LLM?\"\nmessages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}]\nresponse = client.chat.completions.create(\n    model=\"TinyLLM\",\n    messages=messages,\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"local/#local-frameworks","title":"Local Frameworks","text":""},{"location":"local/#llamaindex","title":"LLamaIndex","text":""},{"location":"local/#implementation-steps","title":"Implementation Steps","text":"<ul> <li>Install required packages <pre><code># core package for LlamaIndex, which provides the framework for building and querying indexes\npip install llama-index\n# package for integrating LLaMA models with LlamaIndex, allowing for the use of LLaMA models as the language model (LLM) component\npip install llama-index-llms-llamafile\n# package for providing the embedding functionality using LLaMA models, which is essential for creating vector representations of documents\npip install llama-index-embeddings-llamafile\n# package for openai integration (required only if using openai models)\npip install llama-index-llms-openai\n</code></pre></li> <li>Set up LlamaIndex with LLaMAFile</li> <li>Define custom prompt using prompt templates (Optional)</li> <li>Load Local Data</li> <li>Build the Index</li> <li>Create a Query Engine</li> <li>Query the Index</li> </ul> <p>Sample Code</p> <pre><code>from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.embeddings.llamafile import LlamafileEmbedding\nfrom llama_index.llms.llamafile import Llamafile\n\n# configure LlamaIndex to use the LLaMAFile\n# create an embedding model that utilizes the LLaMAFile\nSettings.embed_model = LlamafileEmbedding(base_url=\"http://localhost:8080\")\n# Set up the Llama model as the LLM component of LlamaIndex\nSettings.llm = Llamafile(base_url=\"http://localhost:8080\", temperature=0, seed=0)\n\n# load local pdf docs\n# the `load_data` method loads the documents from the directory and returns them as a list. \nlocal_reader = SimpleDirectoryReader(input_dir='DirPath/')\ndocs = local_reader.load_data(show_progress=True)\n\n# create an index to store vector representations of the loaded documents. \n# the `from_documents` method builds the index from the provided list of documents.\nindex_pdf = VectorStoreIndex.from_documents(docs)\n\n# convert the index into a query engine that can handle queries and query the index\nquery_engine_pdf = index_pdf.as_query_engine()\n\n# use the query engine to retrieve relevant information from the index\nquery = \"What is the main topic of the document?\"\nresponse = query_engine.query(query)\nprint(response)\n</code></pre>"},{"location":"local/#custom-prompts","title":"Custom Prompts","text":"<p>The most commonly used prompts are <code>text_qa_template</code> (1) and <code>refine_template</code> (2).</p> <ol> <li> Used to get an initial answer to a query using retrieved nodes</li> <li> Used when the retrieved text does not fit into a single LLM call with <code>response_mode=\"compact\"</code> (the default), or when more than one node is retrieved using <code>response_mode=\"refine\"</code><ul> <li>The answer from the first query is inserted as an existing_answer, and the LLM must update or repeat the existing answer based on the new context.</li> </ul> </li> </ol> Sample Code with Custom Prompts Completion PromptsChat Prompts <pre><code>from llama_index.core import PromptTemplate\n\ntext_qa_template_str = (\n    \"Context information is below.\\n---------------------\\n{context_str}\"\n    \"\\n---------------------\\nUsing both the context information and also using\"\n    \" your own knowledge, answer the question: {query_str}\\nIf the context isn't\"\n    \" helpful, you can also answer the question on your own.\\n\"\n)\ntext_qa_template = PromptTemplate(text_qa_template_str)\n\nrefine_template_str = (\n    \"The original question is as follows: {query_str}\\nWe have provided an existing answer: \"\n    \"{existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed)\"\n    \" with some more context below.\\n------------\\n{context_msg}\\n------------\\nUsing both the\"\n    \" new context and your own knowledge, update or repeat the existing answer.\\n\"\n)\nrefine_template = PromptTemplate(refine_template_str)\n\nprint(index_pdf.as_query_engine(\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n        ).query(\"What is the main topic of the document?\"))\n</code></pre> <pre><code>from llama_index.core.llms import ChatMessage, MessageRole\nfrom llama_index.core import ChatPromptTemplate\n\nqa_prompt_str = (\n    \"Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\n\"\n    \"Given the context information and not prior knowledge, answer the question: {query_str}\\n\"\n)\n\nrefine_prompt_str = (\n    \"We have the opportunity to refine the original answer (only if needed) with some more context below.\\n\"\n    \"------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original\"\n    \" answer to better answer the question: {query_str}. If the context isn't useful, output\"\n    \" the original answer again.\\nOriginal Answer: {existing_answer}\"\n)\n\n# Text QA Prompt\nchat_text_qa_msgs = [\n    (\"system\", \"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", qa_prompt_str),\n]\ntext_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n\n# Refine Prompt\nchat_refine_msgs = [\n    (\"system\",\"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", refine_prompt_str),\n]\nrefine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)\n\nprint(index_pdf.as_query_engine(\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n        ).query(\"What is the main topic of the document?\"))\n</code></pre>"},{"location":"local/#langchain","title":"LangChain","text":""},{"location":"local/#implementation-steps_1","title":"Implementation Steps","text":"<ul> <li>Install required packages <pre><code>pip install -U langchain langchain-openai langchain-community\n</code></pre></li> <li>Set up Langchain with Llamafile</li> <li>Create prompt</li> <li>Invoke prompt using model</li> </ul> <p>Sample Code</p> <pre><code>from langchain_community.llms.llamafile import Llamafile\nfrom langchain.prompts import PromptTemplate\n\n# Initialize Llamafile\nllm = Llamafile(temperature=0.7)\n\n# Define prompt\nprompt_template = PromptTemplate.from_template(\"What are LLMs?\")\n\n# Invoke prompt\nprompt = prompt_template.invoke({})\nresult = llm.invoke(prompt)\n</code></pre> <ul> <li> <p>For using local files as context:</p> <ul> <li> <p>Load local data and split into manageable chunks</p> Splitters <p>CharacterTextSplitter:</p> <ul> <li>Tries to preserve paragraphs, sentences, and words as coherent units.</li> <li>Can specify chunk_size, chunk_overlap, and separator.</li> <li>Does not automatically handle very large chunks; instead, it relies on the user setting appropriate values for chunk_size and chunk_overlap.</li> </ul> <p>RecursiveCharacterTextSplitter:</p> <ul> <li>Similar to CharacterTextSplitter, but adds recursive splitting capabilities.</li> <li>Automatically handles very large chunks by attempting to split them according to the specified chunk_size and separator list.</li> <li>If a chunk remains too large after the first round of splitting, it will try again with subsequent separators in the list.</li> </ul> Loading Files <pre><code>from langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n\n# Load local data\npdf_path = 'DirPath/Filename'\npdf_loader = PyPDFLoader(file_path = pdf_path)\n\n# Split using Recursive splitter\n# split is based on the specified chunk size\nsplitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 0)\n\npdf_data_RS = pdf_loader.load_and_split(text_splitter=splitter)\n</code></pre> </li> <li> <p>Define custom prompt using prompt templates (Optional)</p> </li> <li>Instantiate chain</li> <li>Generate response by invoking chain</li> </ul> Sample Code with Custom Prompts Completion PromptsChat Prompts <pre><code>from langchain.chains.summarize import load_summarize_chain\n# create prompt\ntemplate = \"\"\"\nWrite a summary that highlights the main ideas in 3 bullet points of the following:\n\"{text}\"\nSUMMARY:\n\"\"\"\n# Create prompt\nprompt = PromptTemplate.from_template(template)\n# Instantiate chain\nchain = load_summarize_chain(\n    llm=llm,\n    chain_type='stuff',\n    prompt=prompt,\n    verbose=False   # Setting this to true will print the formatted prompt\n)\n# Invoke chain\nresults = chain.invoke(pdf_data_RS)\nprint(f\"Result Keys: {results.keys()}\")\nprint(f\"\\nOutput: {results['output_text']}\")\n</code></pre> <pre><code>from langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define prompt\nprompt = ChatPromptTemplate.from_messages(\n    [(\"system\", \"Summarize the highlights of the following in 3 bullet points:\\\\n\\\\n{context}\")]\n)\n\n# Instantiate chain\nchain = create_stuff_documents_chain(llm, prompt)\n\n# Invoke chain\nresult = chain.invoke({\"context\": pdf_data_RS[1:5]})\n</code></pre> </li> </ul>"},{"location":"nlp/","title":"NLP Models","text":""},{"location":"nlp/#model-types","title":"Model Types","text":""},{"location":"nlp/#bag-of-words","title":"Bag of Words","text":"<p>Sample Code</p> <p>NTLK Corpus</p> <p>NTLK Stem</p> <p>Count Vectorizer</p> <p>Notebook</p> <pre><code>import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer   # 'extracts' the root of the words from their variations\n\n# Clean Data and Build Corpus\ncorpus = []\nps = PorterStemmer()\n\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\n\nfor i in range(0, 1000):\n    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])  # replace all punctuations by space\n    review = review.lower()    # convert to lowercase\n    review = review.split()    # split each review into list of words\n\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    review = ' '.join(review)   # join back the stemmed words for the review\n    corpus.append(review)\n\n# Build Model\nfrom sklearn.feature_extraction.text import CountVectorizer\n# max features set to a number lower than the total number of words so that it excludes the sparsely occuring words\n# total number of words can be found by checking the length of X after the fit transform step\nc_cv = CountVectorizer(max_features = 1500)  \nX = c_cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Train Model and Predict\n# This example uses Gaussian NB\n# Other classification models can also be used\nfrom sklearn.naive_bayes import GaussianNB\nc_gnb = GaussianNB()\nc_gnb.fit(X_train, y_train)\ny_pred_gnb = c_gnb.predict(X_test)\n</code></pre>"},{"location":"nlp/#latent-dirichlet-allocation-lda","title":"Latent Dirichlet Allocation (LDA)","text":"<p>Sample Code (1)</p> <pre><code>from sklearn.decomposition import LatentDirichletAllocation\n\n# Define X and y.\nX = df_yelp.review\ny = df_yelp.stars\n\n# Split the new DataFrame into training and testing sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25 ,random_state=13)\n\n# Use CountVectorizer to create document-term matrices from X_train and X_test.\nvect = CountVectorizer(lowercase=False, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\nX_train_dtm = vect.fit_transform(X_train)\n\nnumber_of_topics = 10\n\nmodel = LatentDirichletAllocation(n_components=number_of_topics, random_state=13)\nmodel.fit(X_train_dtm)\n\nno_top_words = 10\ntopic_dict = {}\nfor topic_idx, topic in enumerate(model.components_):\n    topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(vect.get_feature_names_out()[i])\n                    for i in topic.argsort()[:-no_top_words - 1:-1]]\n    topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n                    for i in topic.argsort()[:-no_top_words - 1:-1]]\n\ntopic_df = pd.DataFrame(topic_dict)\n</code></pre> <ol> <li> <p>LatentDirichletAllocation</p> <p>Notebook</p> </li> </ol>"},{"location":"nlp/#bertopic","title":"BERTopic","text":"<p>Sample Code</p> <p>BERTopic</p> <p>Notebook</p> <pre><code>from bertopic import BERTopic\n\nnlp = spacy.load('en_core_web_sm', exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n\ntopic_model = BERTopic(embedding_model=nlp)\ntopics, probs = topic_model.fit_transform(df_yelp['review'])\n\ntopic_model.get_topic_info()\n</code></pre>"},{"location":"nlpb/","title":"NLP Modeling Basics","text":"Useful Libraries and Packages <ul> <li> <p>Natural Language Toolkit (NLTK) - NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries</p> <ul> <li>Punkt - In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data. </li> </ul> </li> <li> <p>TextBlob - This library provides a simplified interface for exploring common NLP tasks including part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation etc.</p> </li> <li> <p>spaCy - spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It helps build applications that process and \u201cunderstand\u201d large volumes of text. It can be used to build information extraction or natural language understanding systems.</p> </li> <li> <p>Gensim - It is one of the fastest library for training of vector embeddings . The core algorithms in Gensim use battle-hardened, highly optimized &amp; parallelized C routines. Gensim can process arbitrarily large corpora, using data-streamed algorithms. There are no \"dataset must fit in RAM\" limitations.</p> </li> <li> <p>Universal Dependencies - Universal Dependencies (UD) is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages.</p> </li> <li> <p>BERTopic - BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions</p> </li> </ul>"},{"location":"nlpb/#modeling-steps","title":"Modeling Steps","text":"<ul> <li>Generate or Read data</li> <li>Preprocess and clean data<ul> <li>Remove stopwords</li> <li>Remove links</li> <li>Remove punctuations; keep only alpha characters</li> <li>Remove double spacing</li> <li>Extract word root</li> <li>Convert to lowercase: One common method of reducing the number of features is converting all text to lowercase before generating features <ul> <li>It might be useful not to convert them to lowercase if capitalization matters.</li> </ul> </li> </ul> </li> <li>Split training and test data</li> <li>Vectorize Data</li> <li>Apply ML Classifier<ul> <li>Model Training (<code>model.fit</code>)</li> </ul> </li> <li>Get processing output</li> </ul> <p>Also see General ML Modeling Steps</p>"},{"location":"nlpb/#tokenization","title":"Tokenization","text":""},{"location":"nlpb/#with-bert","title":"With BERT","text":"<p>The BERT tokenizer has an <code>encode</code>function similar to <code>convert_tokens_to_ids</code> that includes special tokens such as (beginning of the sequence) and (end of the sequence). These special tokens help the model understand where a sequence starts and ends. <p>Sample Code</p> <pre><code>from transformers import BertTokenizer\n\n# Initialize pre trained tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",clean_up_tokenization_spaces=True)\n# Tokenize sentence\ntokens = tokenizer.tokenize(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# convert the tokens to their corresponding numerical values\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n# use encode function to convert the tokens to their corresponding numerical values\nenc_token_ids = tokenizer.encode(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# convert ids back to tokens to inspect the special token ids added by the encode function\ndec_tokens = tokenizer.convert_ids_to_tokens(enc_token_ids)\n</code></pre>"},{"location":"nlpb/#with-spacy","title":"With spaCy","text":"<p>On parsing, the following components are identified for each token:</p> <ul> <li><code>Text</code>: The original word text.</li> <li><code>Lemma</code>: The base form of the word.</li> <li><code>POS</code>: The simple UPOS part-of-speech tag.</li> <li><code>Tag</code>: The detailed part-of-speech tag.</li> <li><code>Dep</code>: Syntactic dependency, i.e. the relation between tokens.</li> <li><code>Shape</code>: The word shape \u2013 capitalization, punctuation, digits.</li> <li><code>is alpha</code>: Is the token an alpha character?</li> <li><code>is stop</code>: Is the token part of a stop list, i.e. the most common words of the language?</li> </ul> <p>Also see spaCy Linguistic Annotations</p> <p>Sample Code</p> <pre><code>import spacy\nfrom spacy import displacy   # for vizualizing dependencies\n\n# Load corpus\nnlp = spacy.load(\"en_core_web_sm\")\n# Tokenize text using above corpus\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# Vizualize dependencies\nfrom spacy import displacy\nhtml = displacy.render(doc, style=\"dep\")\n\n# Print parsed building blocks\nfor ix, token in enumerate(doc):\nprint(ix, token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n</code></pre>"},{"location":"nlpb/#spacy-ner","title":"spaCy NER","text":"<p>spaCy can recognize various types of named entities in a document and show the label of the named entity as well as the position of the entity in the document. spaCy NER returns the following:</p> <ul> <li><code>Text</code>: The original entity text.</li> <li><code>Start</code>: Index of start of entity in the Doc.</li> <li><code>End</code>: Index of end of entity in the Doc.</li> <li><code>Label</code>: Entity label, i.e. type.</li> </ul> <p>Sample Code</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# Parse entities using .ents\nfor ix, ent in enumerate(doc.ents):\n    print(ix, ent.text, ent.start_char, ent.end_char, ent.label_)\n</code></pre>"},{"location":"nlpb/#spacy-pipelines","title":"spaCy Pipelines","text":"<p>spaCy uses a specific component such as <code>Tagger</code>, <code>DependencyParser</code>, <code>EntityRecognizer</code>, <code>Lemmatizer</code>, <code>TextCategorizer</code> etc. for these pipelines. Pipeline componets can be added and retrieved using the add_pipe and get_pipe methods respectively. </p> <ul> <li>One of the componets called the AttributeRuler can be used to handle exceptions for token attributes and to map values between attributes such as mapping fine-grained POS tags to coarse-grained POS tags.</li> </ul> <p>Also see spaCy Pipeline documentation and spaCy Processing Pipeline documentation. </p> <p>Sample Code</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I saw The Beatles perform. Who did you see?\")\n\n# Get pipe for lemmatizer\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\nprint(lemmatizer.mode)\n\n# Get pipe for attribute ruler and modify\nruler = nlp.get_pipe(\"attribute_ruler\")\n# Pattern to match \"The Beatles\"\npatterns = [[{\"LOWER\": \"the\"}, {\"TEXT\": \"Beatles\"}]]\n# The attributes to assign to the matched token\nattrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n# Add rules to the attribute ruler\nruler.add(patterns=patterns, attrs=attrs, index=0)  # \"The\" in \"The Beatles\"\nruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Beatles\" in \"The Beatles\"\n</code></pre>"},{"location":"nlpb/#vectorization","title":"Vectorization","text":"<p>Sample Code</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\nc_cv = CountVectorizer()\nX = c_cv.fit_transform(df_yelp.text)\n\n# Last 10 features\nprint((c_cv.get_feature_names_out()[-10:]))\n\n# print first 10 items in the vocab\nfrom itertools import islice\nlist(islice(c_cv.vocabulary_.items(), 10))\n</code></pre>"},{"location":"nlpb/#embeddings","title":"Embeddings","text":"<p>Sample Code</p> <pre><code>import torch\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# get the embedding vector for the word \"apple\"\ntoken_id_apple = tokenizer.convert_tokens_to_ids([\"apple\"])[0]\nembedding_apple = model.embeddings.word_embeddings(torch.tensor([token_id_apple]))\n\n# get the embedding vector for the word \"orange\"\ntoken_id_orange = tokenizer.convert_tokens_to_ids([\"orange\"])[0]\nembedding_orange = model.embeddings.word_embeddings(torch.tensor([token_id_orange]))\n\n# Check similarity between two words based on their embeddings\ncos = torch.nn.CosineSimilarity(dim=1)\nsimilarity = cos(embedding_apple, embedding_orange)\nprint(f\"Cosine Similarity between 'apple' and 'orange': {similarity[0] * 100}%\")\n</code></pre>"}]}